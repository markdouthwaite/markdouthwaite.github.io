<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-01T18:27:13+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Mark Douthwaite</title><subtitle>Blogs, thoughts and noise.</subtitle><author><name>Mark Douthwaite</name></author><entry><title type="html">Reservoir Computing: Turning almost any object into a computer</title><link href="http://localhost:4000/ReservoirComputers/" rel="alternate" type="text/html" title="Reservoir Computing: Turning almost any object into a computer" /><published>2018-12-01T00:00:00+00:00</published><updated>2018-12-01T00:00:00+00:00</updated><id>http://localhost:4000/ReservoirComputers</id><content type="html" xml:base="http://localhost:4000/ReservoirComputers/">&lt;p&gt;&lt;img src=&quot;https://www.calacademy.org/sites/default/files/assets/images/Education_Images/TYE_Images/earthswater_waterdrop_usdanrcs1200x900.jpg&quot; alt=&quot;droplet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The latest chip in the iPhone 7 has 3.3 billion transistors packed into a piece of silicon around the size of a small coin. But the trend for smaller, increasingly powerful computers could be coming to an end. Silicon-based chips are rapidly reaching a point at which the laws of physics prevent them being any smaller. There are also some important limitations to what silicon-based devices can do that mean there is a strong argument for looking at other ways to power computers.&lt;/p&gt;

&lt;p&gt;Perhaps the most well-known alternative researchers are looking at is quantum computers, which manipulate the properties of the chips in a different way to traditional digital machines. But there is also the possibilty of using alternative materials – potentially any material or physical system – as computers to perform calculations, without the need to manipulate electrons like silicon chips do. And it turns out these could be even better for developing artificial intelligence than existing computers.&lt;/p&gt;

&lt;p&gt;The idea is commonly known as “reservoir computing” and came from attempts to develop computer networks modelled on the brain. It involves the idea that we can tap into the behaviour of physical systems – anything from a bucket of water to blobs of plastic laced with carbon nanotubes – in order to harness their natural computing power.&lt;/p&gt;

&lt;h2 id=&quot;input-and-output&quot;&gt;Input and output&lt;/h2&gt;

&lt;p&gt;Reservoir computers exploit the physical properties of a material in its natural state to do part of a computation. This contrasts with the current digital computing model of changing a material’s properties to perform computations. For example, to create modern microchips we alter the crystal structure of silicon. A reservoir computer could, in principle, be made from a piece of silicon (or any number of other materials) without these design modifications.&lt;/p&gt;

&lt;p&gt;The basic idea is to stimulate a material in some way and learn to measure how this affects it. If you can work out how you get from the input stimulation to the output change, you will effectively have a calculation that you can then use as part of a range of computations. Unlike with traditional computer chips that depend on the position of electrons, the specific arrangement of the particles in the material isn’t important. Instead we just need to observe certain overall properties that let us measure the output change in the material.&lt;/p&gt;

&lt;p&gt;For example, one team of researchers has built a simple reservoir computer out of a bucket of water. They demonstrated that, after stimulating the water with mechanical probes, they could train a camera watching the water’s surface to read the distinctive ripple patterns that formed. They then worked out the calculation that linked the probe movements with the ripple pattern, and then used it to perform some simple logical operations. Fundamentally, the water itself was transforming the input from the probes into a useful output –- and that is the great insight.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://images.theconversation.com/files/141201/original/image-20161011-3894-1kxax9j.jpg?ixlib=rb-1.1.0&amp;amp;q=30&amp;amp;auto=format&amp;amp;w=600&amp;amp;h=428&amp;amp;fit=crop&amp;amp;dpr=2&quot; alt=&quot;Generic Brain Image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;general-purpose-brain-cells&quot;&gt;General purpose brain cells&lt;/h2&gt;

&lt;p&gt;It turns out that this idea of reservoir computing aligns with recent neuroscience research that discovered parts of the brain appear to be “general-purpose”. These areas are predominantly made up of collections of neurons that are only loosely ordered yet can still support cognitive function in more specialised parts of the brain, helping to make it more efficient. As with the computer, if this reservoir of neurons is stimulated with a specific signal it will respond in a very characteristic way, and this response can help perform computations.&lt;/p&gt;

&lt;p&gt;For example, recent work suggests that when we hear or see something, one general part of the brain is stimulated by sound or light. The response of the neurons in that area of the brain is then read by another more specialised area of the brain.&lt;/p&gt;

&lt;p&gt;Research indicates that reservoir computers could be extremely robust and computationally powerful and, in theory, could effectively carry out an infinite number of functions. In fact, simulated reservoirs have already become very popular in some aspects of artificial intelligence thanks to precisely these properties. For example, systems using reservoir methods for making stock-market predictions have indicated that they outperform many conventional artificial intelligence technologies. In part, this is because it turns out to be much easier to train AI that harnesses the power of a reservoir than one that does not.&lt;/p&gt;

&lt;p&gt;Ultimately, this is still a relatively new technology and a good deal of research remains to be done into its capabilities and implications. But it is already clear that there are a huge number of potential applications of this type of technology both in AI and more broadly. This could include anything from analysing and processing real-time data to image/pattern recognition and controlling robots.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Originally posted on The Conversation, &lt;a href=&quot;https://theconversation.com/theres-a-way-to-turn-almost-any-object-into-a-computer-and-it-could-cause-shockwaves-in-ai-62235&quot;&gt;here&lt;/a&gt; in collaboration with Matt Dale.&lt;/em&gt;&lt;/p&gt;</content><author><name>Mark Douthwaite</name></author><category term="artificial intelligence" /><category term="engineering" /><summary type="html">Reservoir computers exploit the physical properties of a material in its natural state to do part of a computation. This contrasts with the current digital computing model of changing a material’s properties to perform computations. For example, to create modern microchips we alter the crystal structure of silicon. A reservoir computer could, in principle, be made from a piece of silicon (or any number of other materials) without these design modifications.</summary></entry><entry><title type="html">Python Utilities for Data Scientists - Part 1: tqdm</title><link href="http://localhost:4000/NonDataSciencePackagesForDataSciencept1tqdm/" rel="alternate" type="text/html" title="Python Utilities for Data Scientists - Part 1: tqdm" /><published>2018-12-01T00:00:00+00:00</published><updated>2018-12-01T00:00:00+00:00</updated><id>http://localhost:4000/NonDataSciencePackagesForDataSciencept1tqdm</id><content type="html" xml:base="http://localhost:4000/NonDataSciencePackagesForDataSciencept1tqdm/">&lt;p&gt;This series of posts is going to give an overview of some interesting, useful and/or time-saving general-purpose packages that can make the lives of Data Scientists a little easier. I thought I’d put this together after finding a few-to-many DS repositories, scripts and notebooks that made life far too difficult for themselves.&lt;/p&gt;

&lt;h2 id=&quot;tqdm&quot;&gt;TQDM&lt;/h2&gt;

&lt;p&gt;To kick things off, let’s look at &lt;code class=&quot;highlighter-rouge&quot;&gt;tqdm&lt;/code&gt;, a fantastic, easy-to-use, extensible progress bar package. It makes adding fast, informative progress bars to Python processes extremely easy. If you’re a Data Scientist or Machine Learning (ML) Engineer with any degree of experience, you’ll no doubt have used or developed algorithms or data transformations that can take many hours to complete.&lt;/p&gt;

&lt;p&gt;Invariably, many Data Scientists opt to simply print status messages to console, or in some slightly more sophisticated cases use the (excellent and recommended) built-in &lt;code class=&quot;highlighter-rouge&quot;&gt;logging&lt;/code&gt; module. In a lot of cases this is fine. However, if you’re running a task with many hundreds of steps, or over a data structure with many millions of elements, these approaches are sometimes a little unclear and gratuitous, and frankly kind of ugly.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;http://mark.douthwaite.io/wp-content/uploads/2018/11/Screenshot-2018-11-26-at-22.20.37-1024x104.png&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; srcset=&quot;http://mark.douthwaite.io/wp-content/uploads/2018/11/Screenshot-2018-11-26-at-22.20.37-1024x104.png 1024w, http://mark.douthwaite.io/wp-content/uploads/2018/11/Screenshot-2018-11-26-at-22.20.37-300x31.png 300w, http://mark.douthwaite.io/wp-content/uploads/2018/11/Screenshot-2018-11-26-at-22.20.37-768x78.png 768w, http://mark.douthwaite.io/wp-content/uploads/2018/11/Screenshot-2018-11-26-at-22.20.37.png 1080w&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;104&quot; /&gt;&lt;figcaption&gt;Example output from &lt;code class=&quot;prettyprint&quot;&gt;tqdm&lt;/code&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;That’s where &lt;code class=&quot;highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; can come in. It has a nice clean API that lets us quickly add progress bars to our code. Plus it has a lightweight ‘time-remaining’ estimation algorithm built in to the progress bar too. When I first came across &lt;code class=&quot;highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; (while exploring the &lt;code class=&quot;highlighter-rouge&quot;&gt;implicit&lt;/code&gt; library), my mind immediately went to how I could use it to make my own ML algorithm outputs a lot tidier. Take a look at the example below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;desc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training Epoch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;progress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;progress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# increments the progress bar
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this simple example, we set up a &lt;code class=&quot;highlighter-rouge&quot;&gt;tqdm&lt;/code&gt; progress bar that expects a process of 100 steps. We then run our mock training loop, each time updating the progress bar when the step is completed. We can also update the progress bar by arbitrary amounts if we break out of the loop too. That’s two lines of code (plus the import statement) to get a rich progress bar in your code.&lt;/p&gt;

&lt;h2 id=&quot;pandas-integration&quot;&gt;Pandas Integration&lt;/h2&gt;

&lt;p&gt;Beyond cool little additions to your program’s outputs,&lt;code class=&quot;highlighter-rouge&quot;&gt;tqdm&lt;/code&gt;&amp;lt;/code&amp;gt;`also integrates nicely with other widely used packages. Probably the most interesting integration for Data Scientists is with Pandas, the ubiquitous Python data analysis library. Take a look at the example below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;weather.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;desc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Applying Transformation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;progress_apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This would give us:&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;http://mark.douthwaite.io/wp-content/uploads/2018/11/Screenshot-2018-12-01-at-12.41.27.png&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;104&quot; /&gt;&lt;figcaption&gt;Example output from `tqdm`&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Technically, the &lt;code class=&quot;highlighter-rouge&quot;&gt;tqdm.pandas&lt;/code&gt; method monkey patches the &lt;code class=&quot;highlighter-rouge&quot;&gt;progress_apply&lt;/code&gt; method onto Pandas data structures, giving them a modified version of the commonly used &lt;code class=&quot;highlighter-rouge&quot;&gt;apply&lt;/code&gt; method. Practically, when we call the &lt;code class=&quot;highlighter-rouge&quot;&gt;progress_apply&lt;/code&gt; method, the package wraps the standard Pandas ‘apply’ method with a tqdm progress bar. This can come in really handy when you’re processing large data frames!&lt;/p&gt;</content><author><name>Mark Douthwaite</name></author><category term="data science" /><category term="python" /><summary type="html">Data Scientists can sometimes make life difficult for themselves simply because the focus of their work is relatively narrow in comparison to the capabilities of a language like Python. This series of posts looks at general-purpose packages that may save Data Scientists a fair bit of time.</summary></entry></feed>